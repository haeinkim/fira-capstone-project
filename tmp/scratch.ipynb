{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.define_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation1': <HDF5 dataset \"stack\": shape (520, 520, 520), type \"<i8\">}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_volumes = 'validation1:third_party/neuroproof_examples/validation_sample/groundtruth.h5:stack'\n",
    "\n",
    "label_volume_map = {}\n",
    "for vol in label_volumes.split(','):\n",
    "    volname, path, dataset = vol.split(':')\n",
    "    label_volume_map[volname] = h5py.File(path)[dataset]\n",
    "\n",
    "label_volume_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inputs.load_patch_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_file_pattern = 'gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-*-of-00025.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename_queue(coordinates_file_pattern, shuffle=True):\n",
    "    \"\"\"Creates a queue for reading coordinates from coordinate file.\n",
    "\n",
    "    Args:\n",
    "    coordinates_file_pattern: File pattern for TFRecords of\n",
    "                              input examples of the form of a glob\n",
    "                              pattern or path@shards.\n",
    "    shuffle: Whether to shuffle the coordinate file list. Note that the expanded\n",
    "             coordinates_file_pattern is not guaranteed to be sorted\n",
    "             alphabetically.\n",
    "\n",
    "    Returns:\n",
    "    Tensorflow queue with coordinate filenames\n",
    "    \"\"\"\n",
    "    m = re.search(r'@(\\d{1,})', coordinates_file_pattern)\n",
    "    \n",
    "    if m:\n",
    "        num_shards = int(m.group(1))\n",
    "        coord_file_list = [\n",
    "            re.sub(r'@(\\d{1,})', '-%.5d-of-%.5d' % (i, num_shards), \n",
    "                   coordinates_file_pattern)\n",
    "        for i in range(num_shards)]\n",
    "    \n",
    "    else:\n",
    "        coord_file_list = gfile.Glob(coordinates_file_pattern)\n",
    "\n",
    "    return tf.train.string_input_producer(coord_file_list, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-00020-of-00025.gz',\n",
       " 'gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-00021-of-00025.gz',\n",
       " 'gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-00022-of-00025.gz',\n",
       " 'gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-00023-of-00025.gz',\n",
       " 'gs://ffn-flyem-fib25/validation_sample/fib_flyem_validation1_label_lom24_24_24_part14_wbbox_coords-00024-of-00025.gz']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_file_list = gfile.Glob(coordinates_file_pattern)\n",
    "coord_file_list[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x7f8bcba44a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_queue = create_filename_queue(coordinates_file_pattern, shuffle=True)\n",
    "filename_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patch_coordinates_from_filename_queue(filename_queue):\n",
    "    \"\"\"Loads coordinates and volume names from filename queue.\n",
    "\n",
    "    Args:\n",
    "    filename_queue: Tensorflow queue created from create_filename_queue()\n",
    "\n",
    "    Returns:\n",
    "    Tuple of coordinates (shape `[1, 3]`) and volume name (shape `[1]`) tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    record_options = tf.python_io.TFRecordOptions(\n",
    "        tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    keys, protos = tf.TFRecordReader(options=record_options).read(filename_queue)\n",
    "    examples = tf.parse_single_example(protos, features=dict(\n",
    "        center=tf.FixedLenFeature(shape=[1, 3], dtype=tf.int64),\n",
    "        label_volume_name=tf.FixedLenFeature(shape=[1], dtype=tf.string),\n",
    "    ))\n",
    "    coord = examples['center']\n",
    "    volname = examples['label_volume_name']\n",
    "    \n",
    "    return coord, volname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: Tensor(\"ReaderReadV2:0\", shape=(), dtype=string)\n",
      "\n",
      "protos: Tensor(\"ReaderReadV2:1\", shape=(), dtype=string)\n",
      "\n",
      "examples: {'center': <tf.Tensor 'ParseSingleExample/Squeeze_center:0' shape=(1, 3) dtype=int64>, 'label_volume_name': <tf.Tensor 'ParseSingleExample/Squeeze_label_volume_name:0' shape=(1,) dtype=string>}\n",
      "\n",
      "coord: Tensor(\"ParseSingleExample/Squeeze_center:0\", shape=(1, 3), dtype=int64)\n",
      "\n",
      "volname: Tensor(\"ParseSingleExample/Squeeze_label_volume_name:0\", shape=(1,), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "record_options = tf.python_io.TFRecordOptions(\n",
    "    tf.python_io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "keys, protos = tf.TFRecordReader(options=record_options).read(filename_queue)\n",
    "print('keys: ' + str(keys) + '\\n')\n",
    "print('protos: ' + str(protos) + '\\n')\n",
    "\n",
    "examples = tf.parse_single_example(protos, features=dict(\n",
    "    center=tf.FixedLenFeature(shape=[1, 3], dtype=tf.int64),\n",
    "    label_volume_name=tf.FixedLenFeature(shape=[1], dtype=tf.string),\n",
    "))\n",
    "\n",
    "print('examples: ' + str(examples) + '\\n')\n",
    "\n",
    "coord = examples['center']\n",
    "volname = examples['label_volume_name']\n",
    "\n",
    "print('coord: ' + str(coord) + '\\n')\n",
    "print('volname: ' + str(volname) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ParseSingleExample_1/Squeeze_center:0' shape=(1, 3) dtype=int64>,\n",
       " <tf.Tensor 'ParseSingleExample_1/Squeeze_label_volume_name:0' shape=(1,) dtype=string>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_patch_coordinates_from_filename_queue(filename_queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patch_coordinates(coordinates_file_pattern,\n",
    "                           shuffle=True,\n",
    "                           scope='load_patch_coordinates'):\n",
    "    \"\"\"Loads coordinates and volume names from tables of VolumeStoreInputExamples.\n",
    "\n",
    "    Args:\n",
    "    coordinates_file_pattern: File pattern for TFRecords of\n",
    "                              input examples of the form of a glob\n",
    "                              pattern or path@shards.\n",
    "    shuffle: Whether to shuffle the coordinate file list. Note that the expanded\n",
    "             coordinates_file_pattern is not guaranteed to be sorted\n",
    "             alphabetically.\n",
    "    scope: Passed to name_scope.\n",
    "\n",
    "    Returns:\n",
    "    Tuple of coordinates (shape `[1, 3]`) and volume name (shape `[1]`) tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        filename_queue = create_filename_queue(\n",
    "        coordinates_file_pattern, shuffle=shuffle)\n",
    "        \n",
    "    return load_patch_coordinates_from_filename_queue(filename_queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ParseSingleExample_2/Squeeze_center:0' shape=(1, 3) dtype=int64>,\n",
       " <tf.Tensor 'ParseSingleExample_2/Squeeze_label_volume_name:0' shape=(1,) dtype=string>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord, volname = load_patch_coordinates(coordinates_file_pattern)\n",
    "coord, volname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordin = np.array([[128, 128, 128]])\n",
    "coordin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(coord[0, 0:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inputs.load_from_numpylike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_numpylike(coordinates, volume_names, shape, volume_map,\n",
    "                        name=None):\n",
    "    \"\"\"TensorFlow Python op that loads data from Numpy-like volumes.\n",
    "\n",
    "    The volume object must support Numpy-like indexing, as well as shape, ndim,\n",
    "    and dtype properties.  The volume can be 3d or 4d.\n",
    "\n",
    "    Args:\n",
    "    coordinates: tensor of shape [1, 3] containing XYZ coordinates of the\n",
    "        center of the subvolume to load.\n",
    "    volume_names: tensor of shape [1] containing names of volumes to load data\n",
    "        from.\n",
    "    shape: a 3-sequence giving the XYZ shape of the data to load.\n",
    "    volume_map: a dictionary mapping volume names to volume objects.  See above\n",
    "        for API requirements of the Numpy-like volume objects.\n",
    "    name: the op name.\n",
    "\n",
    "    Returns:\n",
    "    Tensor result of reading data of shape [1] + shape[::-1] + [num_channels]\n",
    "    from given center coordinate and volume name.  Dtype matches input volumes.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if volumes in volume_map have inconsistent dtypes or number of\n",
    "    channels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _num_channels(volume):\n",
    "        if volume.ndim == 3:\n",
    "            return 1\n",
    "        return volume.shape[0]\n",
    "\n",
    "  # Validate that all volumes are compatible.\n",
    "    volumes = iter(volume_map.values())\n",
    "    first_vol = next(volumes)\n",
    "    dtype = first_vol.dtype\n",
    "    num_channels = _num_channels(first_vol)\n",
    "    for volume in volumes:\n",
    "        if volume.dtype != dtype:\n",
    "            raise ValueError('All volumes should have same dtype.')\n",
    "        if _num_channels(volume) != num_channels:\n",
    "            raise ValueError('All volumes should have same number of channels.')\n",
    "\n",
    "    start_offset = (np.array(shape) - 1) // 2\n",
    "    def _load_from_numpylike(coord, volname):\n",
    "        \"\"\"Load from coord and volname, handling 3d or 4d volumes.\"\"\"\n",
    "        volume = volume_map[volname.decode('ascii')]\n",
    "        # Get data, including all channels if volume is 4d.\n",
    "        starts = np.array(coord) - start_offset\n",
    "        slc = bounding_box.BoundingBox(start=starts, size=shape).ToSlice()\n",
    "        if volume.ndim == 4:\n",
    "            slc = np.index_exp[:] + slc\n",
    "        data = volume[slc]\n",
    "\n",
    "        # If 4d, move channels to back.  Otherwise, just add flat channels dim.\n",
    "        if data.ndim == 4:\n",
    "            data = np.rollaxis(data, 0, start=4)\n",
    "        else:\n",
    "            data = np.expand_dims(data, 4)\n",
    "\n",
    "        # Add flat batch dim and return.\n",
    "        data = np.expand_dims(data, 0)\n",
    "        return data\n",
    "\n",
    "    with tf.name_scope(name, 'LoadFromNumpyLike',\n",
    "                       [coordinates, volume_names]) as scope:\n",
    "        # For historical reasons these have extra flat dims.\n",
    "        coordinates = tf.squeeze(coordinates, axis=0)\n",
    "        volume_names = tf.squeeze(volume_names, axis=0)\n",
    "\n",
    "        loaded = tf.py_func(\n",
    "            _load_from_numpylike, [coordinates, volume_names], [dtype],\n",
    "            name=scope)[0]\n",
    "        \n",
    "    loaded.set_shape([1] + list(shape[::-1]) + [num_channels])\n",
    "    return loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _num_channels(volume):\n",
    "    if volume.ndim == 3:\n",
    "        return 1\n",
    "    return volume.shape[0]\n",
    "\n",
    "# Validate that all volumes are compatible.\n",
    "volumes = iter(label_volume_map.values())\n",
    "first_vol = next(volumes)\n",
    "dtype = first_vol.dtype\n",
    "num_channels = _num_channels(first_vol)\n",
    "for volume in volumes:\n",
    "    if volume.dtype != dtype:\n",
    "        raise ValueError('All volumes should have same dtype.')\n",
    "    if _num_channels(volume) != num_channels:\n",
    "        raise ValueError('All volumes should have same number of channels.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volumes)\n",
    "print(first_vol)\n",
    "print(dtype)\n",
    "print(num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = np.array([49, 49, 49])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_offset = (np.array(label_size) - 1) // 2\n",
    "\n",
    "def _load_from_numpylike(coord, volname):\n",
    "    \"\"\"Load from coord and volname, handling 3d or 4d volumes.\"\"\"\n",
    "    volume = volume_map[volname.decode('ascii')]\n",
    "    # Get data, including all channels if volume is 4d.\n",
    "    starts = np.array(coord) - start_offset\n",
    "    slc = bounding_box.BoundingBox(start=starts, size=shape).ToSlice()\n",
    "    if volume.ndim == 4:\n",
    "        slc = np.index_exp[:] + slc\n",
    "    data = volume[slc]\n",
    "\n",
    "    # If 4d, move channels to back.  Otherwise, just add flat channels dim.\n",
    "    if data.ndim == 4:\n",
    "        data = np.rollaxis(data, 0, start=4)\n",
    "    else:\n",
    "        data = np.expand_dims(data, 4)\n",
    "\n",
    "    # Add flat batch dim and return.\n",
    "    data = np.expand_dims(data, 0)\n",
    "    return data\n",
    "\n",
    "with tf.name_scope(None, 'LoadFromNumpyLike',\n",
    "                   [coord, volname]) as scope:\n",
    "    # For historical reasons these have extra flat dims.\n",
    "    coordinates = tf.squeeze(coord, axis=0)\n",
    "    volume_names = tf.squeeze(volname, axis=0)\n",
    "\n",
    "    loaded = tf.py_func(\n",
    "        _load_from_numpylike, [coordinates, volume_names], [dtype],\n",
    "        name=scope)[0]\n",
    "\n",
    "loaded.set_shape([1] + list(label_size[::-1]) + [num_channels])\n",
    "loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffn.utils import bounding_box\n",
    "\n",
    "# If our coordinates are given as: \n",
    "coord = np.array([[128, 128, 128]])\n",
    "\n",
    "starts = coord - start_offset\n",
    "slc = bounding_box.BoundingBox(start=starts, size=label_size)# .ToSlice()\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load_from_numpylike(\n",
    "  coord, volname, label_size, label_volume_map)\n",
    "\n",
    "\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch(iterable):\n",
    "    for batch_vals in iterable:\n",
    "        yield zip(*batch_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example():\n",
    "    while True:\n",
    "        for i in range(27): # offset duplicates\n",
    "#             print(i, \"it is i\")\n",
    "            predicted = np.full((1, 49, 49, 49, 1), 0)\n",
    "            patches = np.full((1, 49, 49, 49, 1), 1)\n",
    "            labels = np.full((1, 49, 49, 49, 1), 2)\n",
    "            yield predicted, patches, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(get_example())[0].shape) # seed array\n",
    "print(next(get_example())[1].shape) # image array\n",
    "print(next(get_example())[2].shape) # label array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six.moves.zip(*[get_example() for _ in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch(six.moves.zip(*[get_example() for _ in range(batch_size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds, patches, labels = next(_batch(six.moves.zip(*[get_example() for _ in range(batch_size)])))\n",
    "\n",
    "len(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    batch_size = 8\n",
    "    step = 0\n",
    "\n",
    "    for seeds, patches, labels in _batch(six.moves.zip(*[get_example() for _ in range(batch_size)])): \n",
    "\n",
    "        batched_seeds = np.concatenate(seeds)\n",
    "        print(\"seeds shape\", len(seeds))\n",
    "\n",
    "        yield (batched_seeds, np.concatenate(patches), np.concatenate(labels))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            print(\"batched_seeds[i, ...]\", i,batched_seeds[i, ...].shape)\n",
    "            seeds[i][:] = batched_seeds[i, ...]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed, patches, labels = next(get_batch())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seed.shape)\n",
    "print(patches.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i, j, k in get_batch():\n",
    "    print(i.shape, j.shape, k.shape)\n",
    "#     tmp.append([i.shape, j.shape, k.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.95\n",
    "\n",
    "seed = np.full((1, 49, 49, 49, 1), 0.05)\n",
    "seed[:, 24, 24, 24, :] = p\n",
    "seed[:, 24, 24, 24, :]\n",
    "logit_seed = logit(seed)\n",
    "logit_seed[:, 24, 24, 24, :]\n",
    "np.where(logit_seed == logit(0.95))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
