{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c.f. It is different from saved_gt  and diced_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a diced_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from diced import DicedStore\n",
    "\n",
    "store = DicedStore(\"gs://flyem-public-connectome\")\n",
    "repo_grayscale = store.open_repo(\"medulla7column\")\n",
    "repo_groundtruth = store.open_repo(\"medulla7column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grayscale = repo_grayscale.get_array(\"grayscale\")\n",
    "groundtruth = repo_groundtruth.get_array(\"groundtruth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 3253\n",
    "xmax = 3773\n",
    "ymin = 2103\n",
    "ymax = 2623\n",
    "zmin = 3490\n",
    "zmax = 4010\n",
    "origin_gt_cube = groundtruth[zmin:zmax, ymin:ymax, xmin:xmax] \n",
    "\n",
    "unique_origin_gt_cube = np.unique(origin_gt_cube)\n",
    "origin_gt_cube.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a saved_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# conda install -c anaconda h5py\n",
    "\"\"\"\n",
    "Reference : https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr\n",
    "Saving 'thousand_gry' as np.array-from, its size is 2.7GB\n",
    "but saving 'thousand_gry' as hdf-from, its size is 1.1GB\n",
    "\"\"\"\n",
    "def hdf_read(path, keyword):\n",
    "    f = h5py.File(path, 'r')\n",
    "    return f[keyword]\n",
    "\n",
    "def hdf_write(path, keyword, data):\n",
    "    with h5py.File(path, 'w') as outfile:\n",
    "        dset = dset = outfile.create_dataset(keyword, data=data, chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = '/data/git/ffn/third_party/neuroproof_examples/validation_sample/groundtruth.h5'\n",
    "saved_gt = np.array(hdf_read(gt_path, 'stack'))\n",
    "\n",
    "unique_saved_gt = np.unique(saved_gt)\n",
    "saved_gt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check a saved_gt same with a diced_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage import color\n",
    "\n",
    "z_value = 30\n",
    "\n",
    "color_labels = color.label2rgb(origin_gt_cube[z_value, :, :])\n",
    "plt.imshow(color_labels, alpha=0.3)\n",
    "# plt.savefig('raw+gt_%d_by_google.png' % z_value, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_labels = color.label2rgb(saved_gt[z_value, :, :], alpha=0.1)\n",
    "\n",
    "plt.imshow(color_labels, alpha=0.3)\n",
    "# plt.savefig('raw+inference_%d_by_google.png' % z_value, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(unique_saved_gt) == len(unique_origin_gt_cube))\n",
    "print(len(unique_saved_gt))\n",
    "print(len(unique_origin_gt_cube))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load a diced_gt_from_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# conda install -c anaconda h5py\n",
    "\"\"\"\n",
    "Reference : https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr\n",
    "Saving 'thousand_gry' as np.array-from, its size is 2.7GB\n",
    "but saving 'thousand_gry' as hdf-from, its size is 1.1GB\n",
    "\"\"\"\n",
    "def hdf_read(path, keyword):\n",
    "    f = h5py.File(path, 'r')\n",
    "    return f[keyword]\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./save/diced_gt_from_server.hdf\"):\n",
    "    with h5py.File(\"./save/diced_gt_from_server.hdf\", \"w\") as outfile:\n",
    "        dset = outfile.create_dataset('diced_gt_from_server', data=origin_gt_cube, chunks=True)\n",
    "    diced_gt = origin_gt_cube\n",
    "else:\n",
    "    gt_path = \"./save/diced_gt_from_server.hdf\"\n",
    "    diced_gt = np.array(hdf_read(gt_path, 'diced_gt_from_server'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neuron coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open roi.json file and specify as a numpy array:\n",
    "\n",
    "with open('json_repo/roi.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "roi_idx = np.array(data)\n",
    "roi_idx[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from 'roi_idx':\n",
    "\n",
    "col_name = ['z_start', 'y_start', 'x0_start', 'x1_start']\n",
    "df = pd.DataFrame(roi_idx, columns=col_name)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate to voxel coorinates:\n",
    "\n",
    "df_coord = df * 32\n",
    "df_coord.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synapse.json file:\n",
    "\n",
    "with open('json_repo/synapse.json', 'r') as f:\n",
    "    synapse_json = json.load(f)\n",
    "    \n",
    "# Get the location of specific 'T-bar': \n",
    "\n",
    "data = synapse_json['data'][22]\n",
    "loc_tbar = data['T-bar']['location']\n",
    "print(\"The voxel coordinates of T-bar is [x, y, z] = \" + str(loc_tbar))\n",
    "\n",
    "# Get the locations of its 'partners':\n",
    "\n",
    "partners = data['partners']\n",
    "loc_part = [partners[i]['location'] for i in range(len(partners))]\n",
    "\n",
    "for i, loc in enumerate(loc_part):\n",
    "    print(\"The voxel coordinates of its partner {} is [x, y, z] = \".format(i) + str(loc_part[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the T-bar locations in synapse.json:\n",
    "\n",
    "data = synapse_json['data']\n",
    "\n",
    "tbar_loc_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tbar = data[i]['T-bar']\n",
    "    x, y, z = tbar['location']\n",
    "    tbar_loc_list.append([i, tbar['body ID'], x, y, z])\n",
    "\n",
    "# Create the DataFrame of T-bar:\n",
    "\n",
    "col_name = ['data', 'body ID', 'loc (x)', 'loc (y)', 'loc (z)']\n",
    "\n",
    "tbar_df = pd.DataFrame(tbar_loc_list, columns=col_name)\n",
    "tbar_df['category'] = 'T-bar'\n",
    "print('The DataFrame of tbar_df would have ' + str(len(tbar_df)) + ' of rows')\n",
    "tbar_df.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the Partners locations corresponding to T-bars in synapse.json:\n",
    "\n",
    "data = synapse_json['data']\n",
    "\n",
    "partners_loc_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    partners = data[i]['partners']\n",
    "    for j in range(len(partners)):\n",
    "        x, y, z = partners[j]['location']\n",
    "        partners_loc_list.append([i, partners[j]['body ID'], x, y, z])\n",
    "    \n",
    "# Create the DataFrame of Partners:\n",
    "\n",
    "col_name = ['data', 'body ID', 'loc (x)', 'loc (y)', 'loc (z)']\n",
    "\n",
    "partner_df = pd.DataFrame(partners_loc_list, columns=col_name)\n",
    "partner_df['category'] = 'partner'\n",
    "\n",
    "print('The DataFrame of partner_df would have ' + str(len(partner_df)) + ' of rows')\n",
    "partner_df.tail(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_df = tbar_df.append(partner_df).reset_index(drop=True)\n",
    "\n",
    "synapse_df.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get location of synapse inside of example:\n",
    "xmin = 3253\n",
    "xmax = 3773\n",
    "ymin = 2103\n",
    "ymax = 2623\n",
    "zmin = 3490\n",
    "zmax = 4010\n",
    "\n",
    "x_cond = (synapse_df['loc (x)'] >= xmin) & (synapse_df['loc (x)'] <= xmax)\n",
    "y_cond = (synapse_df['loc (y)'] >= ymin) & (synapse_df['loc (y)'] <= ymax)\n",
    "z_cond = (synapse_df['loc (z)'] >= zmin) & (synapse_df['loc (z)'] <= zmax)\n",
    "\n",
    "valid_synapse = synapse_df[x_cond & y_cond & z_cond]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "ax.scatter(valid_synapse.iloc[:, 2],\n",
    "           valid_synapse.iloc[:, 3],\n",
    "           valid_synapse.iloc[:, 4], marker='o', label=valid_synapse.iloc[:,5], alpha=0.1)    \n",
    "  \n",
    "ax.set_zlim(zmin, zmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "    \n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def save_object(input_object):\n",
    "    name = namestr(input_object, globals())[0]\n",
    "    with open('./save/' + name + '.dmp', 'wb') as f:\n",
    "        pickle.dump(input_object, f)\n",
    "        \n",
    "def load_object(input_object):\n",
    "    name = namestr(input_object, globals())[0]\n",
    "    with open('./save/' + name + '.dmp', 'rb') as f:\n",
    "        loaded_f = pickle.load(f)\n",
    "    return loaded_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get a neuron_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37a01fcf8720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mneuron_gt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Define a dummy variable to get a variable's name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mneuron_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading neuron_gt complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_object' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./save/neuron_gt.dmp\"):\n",
    "    ##!! First, you have to laod a origin_gt_cube from diced server\n",
    "    # Get neuron_coordinates\n",
    "    neuron_coord_z = valid_synapse['loc (z)'].values\n",
    "    neuron_coord_y = valid_synapse['loc (y)'].values\n",
    "    neuron_coord_x = valid_synapse['loc (x)'].values\n",
    "\n",
    "    assert len(neuron_coord_z) == len(neuron_coord_y) == len(neuron_coord_x)\n",
    "    \n",
    "    # Get a neuron_gt list\n",
    "    neuron_gt = []\n",
    "    for z,y,x in zip(neuron_coord_z, neuron_coord_y, neuron_coord_x):\n",
    "        val = (int)(origin_gt_cube[z:z+1,y:y+1,x:x+1].flatten().tolist()[0])\n",
    "        if not val in neuron_gt:\n",
    "            neuron_gt.append(val)\n",
    "    save_object(neuron_gt)\n",
    "    print(\"saving neuron_gt complete\")\n",
    "else:\n",
    "    neuron_gt=[] # Define a dummy variable to get a variable's name\n",
    "    neuron_gt = load_object(neuron_gt)\n",
    "    print(\"loading neuron_gt complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all \"body ID\" made by neuron_gt\n",
    "neuron_ids = valid_synapse['body ID'].values\n",
    "\n",
    "for i in neuron_ids.tolist():\n",
    "    if not i in neuron_gt:\n",
    "        print(\"%d 없음\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neuron get cube and non_neuron_gt_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_bool = np.isin(diced_gt,neuron_ids)\n",
    "indice = np.where(indice_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diced_gt[0,7,469] in neuron_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neuron_gt_cube = np.zeros((diced_gt.shape), dtype=np.int)\n",
    "# non_neuron_gt_cube = diced_gt.copy()\n",
    "\n",
    "# for z,y,x in zip(indice[0], indice[1], indice[2]):\n",
    "#     neuron_gt_cube[z,y,x] = diced_gt[z,y,x]\n",
    "#     non_neuron_gt_cube[z,y,x] = 0\n",
    "#     if not diced_gt[z,y,x] in neuron_gt :\n",
    "#         print('% d is not in neuron_gt' % diced_gt[z,y,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuron_gt_cube = np.zeros((diced_gt.shape), dtype=np.int)\n",
    "non_neuron_gt_cube = diced_gt.copy()\n",
    "\n",
    "neuron_gt_cube[indice] = diced_gt[indice]\n",
    "non_neuron_gt_cube[indice] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw a neuron scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_gt_cube_path = \"./save/neuron_gt_cube.hdf\"\n",
    "non_neuron_gt_cube_path = \"./save/non_neuron_gt_cube.hdf\"\n",
    "\n",
    "if not (os.path.exists(\"./save/neuron_gt_cube.hdf\") and os.path.exists(\"./save/non_neuron_gt_cube.hdf\")):\n",
    "    hdf_write(neuron_gt_cube_path, \"neuron_gt_cube\", neuron_gt_cube)\n",
    "    hdf_write(non_neuron_gt_cube_path, \"non_neuron_gt_cube\", non_neuron_gt_cube)\n",
    "else:\n",
    "    neuron_gt_cube = np.array(hdf_read(neuron_gt_cube_path, \"neuron_gt_cube\"))\n",
    "    non_neuron_gt_cube = np.array(hdf_read(non_neuron_gt_cube_path, \"non_neuron_gt_cube\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "WARNING: Imported VTK version (7.0) does not match the one used\n",
      "         to build the TVTK classes (6.3). This may cause problems.\n",
      "         Please rebuild TVTK.\n",
      "********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "##!#!#!# Caution #!#!#!##\n",
    "If you run this code, you will be wait long time\n",
    "as you want to run this, unannotate last line\n",
    "'neuron_gt_cube.vtk' is 10.0GB\n",
    "\"\"\"\n",
    "from tvtk.api import tvtk, write_data\n",
    "# conda install -c anaconda mayavi \n",
    "def array2vtk(data, name):\n",
    "    grid = tvtk.ImageData(spacing=(1,1,1), origin=(0, 0, 0), dimensions=data.shape)\n",
    "    grid.point_data.scalars = data.ravel()\n",
    "    grid.point_data.scalars.name = name\n",
    "\n",
    "    write_data(grid, './save/'+ name + '.vtk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array2vtk(neuron_gt_cube, \"neuron_gt_cube\")\n",
    "array2vtk(non_neuron_gt_cube, \"non_neuron_gt_cube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d",
   "language": "python",
   "name": "3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
